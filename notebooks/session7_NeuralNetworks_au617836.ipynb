{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path tools\n",
    "import sys,os\n",
    "sys.path.append(os.path.join(\"..\")) # adding home directory to sys path so we can import the utility function\n",
    "\n",
    "# Neural networks with numpy\n",
    "from utils.neuralnetwork import NeuralNetwork \n",
    "\n",
    "# Sklearn - machine learning tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```utils.neuralnetwork``` is a python class which includes a neural network function called ```NeuralNetwork```. Programming neural networks from the ground up takes a lot of knowledge and many hours, which is why we use this function. We could have used existing Python libraries (e.g. ```tensorflow keras```), but these have something more complex we would need to learn first. Later om we will built our own networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the neural network function form utils to study the handwriting dataset. Rather than working with the full dataset, we are going to use the ```load_digits``` function to only use part of it. This function returns around 180 samples per class from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Some preprocessing__\n",
    "\n",
    "We need to make sure the data is floats rather than ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to floats\n",
    "data = digits.data.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remember min-max regularization?__\n",
    "\n",
    "This is instead of dividing everything by 255. It's a bit of a smarter way to normalize.\n",
    "We use max-min regurlization to normalize each data point, which gives us a more compressed and regular dataset to work with. \n",
    "\n",
    "What happens if you try the other way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax regularization\n",
    "data = (data - data.min())/(data.max() - data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] samples: 1797, dim: 64\n"
     ]
    }
   ],
   "source": [
    "# Print dimensions\n",
    "print(f\"[INFO] samples: {data.shape[0]}, dim: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has around 1800 samples, and the dimensions are 64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, # original data\n",
    "                                                  digits.target, # labels\n",
    "                                                  test_size=0.2) # 20%% of the data goes into the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're converting labels to binary representations.\n",
    "\n",
    "Why am I doing this? Why don't I do it with LogisticRegression() for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels from integers to vectors\n",
    "y_train = LabelBinarizer().fit_transform(y_train) # initializing the binarizer and fit it to the training data\n",
    "y_test = LabelBinarizer().fit_transform(y_test) # doing the same for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^We convert the labels to binary representations using the ```LabelBinarizer``` function. If we had three labels (e.g. cat, dog, mouse), when we binarize them we get only 2 labels (dog = 1, cat = 0, mouse = 0). Hence by binarizing we take the labels and turn them into a binary representation. We do this because classifiers work with 0s and 1s and not string labels. Hence, the output needs to be either a 0 or a 1 even if we have more than two labels. \n",
    "Because we are dealing with multple labels, we need to convert the labels into a binary representaion, to enable the computer to be able to map the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first ten labels now after having binarized them\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the labels have become binarized. Each label is either a 0 or 1. Hence, we have created a binary representation of the labels. This can be done for any kind of labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```NeuralNetwork(input_layer, hidden_layer, output_layer)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes a list of numbers in which each number represent a layer in the neural network. The first layer is the input layer which corresponds to the size of the data. Since the data is 8x8 the input layer should be 64. The output layer is 10 (ten classes that we want to predict). The hidden layer is up to us to specify. We put in 32 for the first hidden layer and 16 for the second hidden layer. These numbers are arbitrary - we decide them. The only real limitation we need to think about is that the sum of the nodes in the hidden layers should be less than the sum of the nodes in the input and output layers. In this case the number of nodes in our hidden layers should not exceed 64 + 10 = 74 nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple networks because we are only defining the number of nodes in the layers and the number of epochs (iterations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! This NeuralNetwork function has a default bias term included. The bias is based on the shape of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "print(\"[INFO] training network...\")\n",
    "nn = NeuralNetwork([X_train.shape[1], 32, 16, 10]) # we specify the input layer, hidden layers, and output layers. The input layer is the size of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can we use X_train.shape[1] to indicate the number of nodes in the input layer?\n",
    "[1] is the number of columns in the data (the number of pixel in each image). For each individual entry in the data, we have an array of 64 values (8x8 - 8 rows and 8 columns) which represent one image. The whole X_train object is a collection of many images that are each 64 pixels. Hence, when we take the entry number 1 we take the number of pixels per image, which is the number of nodes in the input layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] NeuralNetwork: 64-32-16-10\n",
      "[INFO] epoch=1, loss=645.7462609\n",
      "[INFO] epoch=100, loss=7.1322572\n",
      "[INFO] epoch=200, loss=2.5680621\n",
      "[INFO] epoch=300, loss=1.5792949\n",
      "[INFO] epoch=400, loss=1.3613776\n",
      "[INFO] epoch=500, loss=1.2527276\n",
      "[INFO] epoch=600, loss=0.7741279\n",
      "[INFO] epoch=700, loss=0.4473712\n",
      "[INFO] epoch=800, loss=0.2137723\n",
      "[INFO] epoch=900, loss=0.1637844\n",
      "[INFO] epoch=1000, loss=0.1353876\n"
     ]
    }
   ],
   "source": [
    "# Fit network to data - training the network for 1000 epochs (iterations)\n",
    "print(f\"[INFO] {nn}\")\n",
    "nn.fit(X_train, y_train, epochs=1000) # epoch = iteration - a full pass through the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for every epoch we get a lower and lower loss. Hence, each epoch minimizes the loss which is done by learning the weights better and better. For each epoch the model becomes slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How many epochs should one use?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many ways it is through trial-and-error - seeing which model performs best. We could also perform a cross-validation and plot the training score next to the cross-validation score, which allows us to see how well the model converges and whether it underfits or overfits. The number of epochs is dependent on whetehr it is over- or underfitting the data. If the model is underfitting, then you would increase the number of epochs, while if the model is overfitting, you would decrease the number of epochs in order to stop the model because it learns too much from the data.\n",
    "\n",
    "Generally speaking it is either:\n",
    "- Trial-and-error\n",
    "- Educated guess\n",
    "- Computational, algorithm calculation of optimal parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our trained network. Now we can evaluate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model to predict the test class and use the classfication report to produce an output in which we can interpret how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INFO] evaluating network...']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        37\n",
      "           2       1.00      1.00      1.00        37\n",
      "           3       1.00      1.00      1.00        35\n",
      "           4       0.94      1.00      0.97        32\n",
      "           5       0.94      0.94      0.94        35\n",
      "           6       1.00      1.00      1.00        35\n",
      "           7       1.00      0.94      0.97        36\n",
      "           8       1.00      0.95      0.97        37\n",
      "           9       0.92      1.00      0.96        33\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate network\n",
    "print([\"[INFO] evaluating network...\"])\n",
    "predictions = nn.predict(X_test)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(y_test.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this evaluation we can get up to 98% accuracy for these handwritten digits. We can go back to training the model, and adjust the number of epochs for instance and see what that does to the accuracy. \n",
    "With a very small amount of epochs the model might not converge. \n",
    "With this neural network class we can tweek different parameters and get different results and try to get the best model possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you would start out with a logistric regression classifier to get some simple benchmarks, and then you would create a simple neural network and see how these results compare to the benchmark results created by the logistic regression classifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv101",
   "language": "python",
   "name": "cv101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
